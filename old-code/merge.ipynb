{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dataset processing for RUL prediction\n",
    "# Add this after your read_hdf function definition\n",
    "\n",
    "def check_dataset_compatibility(files_to_check):\n",
    "    \"\"\"Check if datasets have compatible shapes and features.\"\"\"\n",
    "    compatibility_info = {}\n",
    "    \n",
    "    for file in files_to_check:\n",
    "        print(f\"\\nChecking {file}...\")\n",
    "        trainx, trainy, adev, testx, testy, atest = read_hdf(filename=file)\n",
    "        \n",
    "        compatibility_info[file] = {\n",
    "            'n_features': trainx.shape[1],\n",
    "            'features': trainx.columns.tolist(),\n",
    "            'train_samples': trainx.shape[0],\n",
    "            'test_samples': testx.shape[0],\n",
    "            'n_units_train': adev['unit'].nunique(),\n",
    "            'n_units_test': atest['unit'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del trainx, trainy, adev, testx, testy, atest\n",
    "        gc.collect()\n",
    "    \n",
    "    return compatibility_info\n",
    "\n",
    "def process_multiple_datasets(file_list, window_size=30, stride=15):\n",
    "    \"\"\"\n",
    "    Process multiple HDF files and combine their data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_list : list\n",
    "        List of file paths to process\n",
    "    window_size : int\n",
    "        Window size for time series slicing\n",
    "    stride : int\n",
    "        Stride for window slicing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Combined training and test data\n",
    "    \"\"\"\n",
    "    \n",
    "    all_train_windows = []\n",
    "    all_train_labels = []\n",
    "    all_test_windows = []\n",
    "    all_test_labels = []\n",
    "    \n",
    "    # Track unit offset to avoid unit ID conflicts across datasets\n",
    "    unit_offset_train = 0\n",
    "    unit_offset_test = 0\n",
    "    \n",
    "    for file_idx, curr_file in enumerate(file_list):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing file {file_idx+1}/{len(file_list)}: {curr_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Read data\n",
    "        trainx, trainy, adev, testx, testy, atest = read_hdf(filename=curr_file)\n",
    "        \n",
    "        # Filter zero RUL values\n",
    "        trainy = trainy[trainy != 0].dropna()\n",
    "        trainx_l = trainx.loc[trainy.index]\n",
    "        testy = testy[testy != 0].dropna()\n",
    "        testx_l = testx.loc[testy.index]\n",
    "        \n",
    "        # Scale data\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(trainx_l.values)\n",
    "        trainx_scaled = sc.transform(trainx_l.values).astype(np.float32)\n",
    "        testx_scaled = sc.transform(testx_l.values).astype(np.float32)\n",
    "        \n",
    "        trainx_scaled = pd.DataFrame(trainx_scaled, columns=trainx.columns)\n",
    "        testx_scaled = pd.DataFrame(testx_scaled, columns=trainx.columns)\n",
    "        \n",
    "        # Process training data\n",
    "        df_train = trainx_scaled.copy()\n",
    "        df_train['unit'] = adev.loc[trainx_scaled.index, 'unit'].values + unit_offset_train\n",
    "        df_train['RUL'] = trainy['RUL'].values\n",
    "        \n",
    "        for un in df_train['unit'].unique():\n",
    "            windows = time_window_slicing_sample(\n",
    "                df_train, window_size, un, \n",
    "                df_train.columns.difference(['unit', 'RUL']), \n",
    "                stride\n",
    "            )\n",
    "            labels = time_window_slicing_label(df_train, window_size, un, stride=stride)\n",
    "            \n",
    "            all_train_windows.append(windows)\n",
    "            all_train_labels.extend(labels.tolist())\n",
    "        \n",
    "        # Process test data\n",
    "        df_test = testx_scaled.copy()\n",
    "        df_test['unit'] = atest.loc[testx_scaled.index, 'unit'].values + unit_offset_test\n",
    "        df_test['RUL'] = testy['RUL'].values\n",
    "        \n",
    "        for un in df_test['unit'].unique():\n",
    "            windows = time_window_slicing_sample(\n",
    "                df_test, window_size, un,\n",
    "                df_test.columns.difference(['unit', 'RUL']),\n",
    "                stride\n",
    "            )\n",
    "            labels = time_window_slicing_label(df_test, window_size, un, stride=stride)\n",
    "            \n",
    "            all_test_windows.append(windows)\n",
    "            all_test_labels.extend(labels.tolist())\n",
    "        \n",
    "        # Update unit offsets\n",
    "        unit_offset_train += df_train['unit'].nunique()\n",
    "        unit_offset_test += df_test['unit'].nunique()\n",
    "        \n",
    "        print(f\"✓ Processed {df_train['unit'].nunique()} training units\")\n",
    "        print(f\"✓ Processed {df_test['unit'].nunique()} test units\")\n",
    "        \n",
    "        # Clean up\n",
    "        del trainx, trainy, adev, testx, testy, atest\n",
    "        del trainx_scaled, testx_scaled, df_train, df_test\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Combining all datasets...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    combined_train_data = np.concatenate([w.transpose(2, 0, 1) for w in all_train_windows], axis=0)\n",
    "    combined_train_labels = np.array(all_train_labels, dtype=np.float32)\n",
    "    \n",
    "    combined_test_data = np.concatenate([w.transpose(2, 0, 1) for w in all_test_windows], axis=0)\n",
    "    combined_test_labels = np.array(all_test_labels, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Combined training data shape: {combined_train_data.shape}\")\n",
    "    print(f\"Combined training labels shape: {combined_train_labels.shape}\")\n",
    "    print(f\"Combined test data shape: {combined_test_data.shape}\")\n",
    "    print(f\"Combined test labels shape: {combined_test_labels.shape}\")\n",
    "    \n",
    "    return combined_train_data, combined_train_labels, combined_test_data, combined_test_labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "# Step 1: Check compatibility\n",
    "print(\"Step 1: Checking dataset compatibility...\")\n",
    "compatibility = check_dataset_compatibility(all_files[:3])  # Check first 3 files\n",
    "\n",
    "# Display compatibility info\n",
    "for file, info in compatibility.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"  Features: {info['n_features']}\")\n",
    "    print(f\"  Training samples: {info['train_samples']}\")\n",
    "    print(f\"  Test samples: {info['test_samples']}\")\n",
    "    print(f\"  Training units: {info['n_units_train']}\")\n",
    "    print(f\"  Test units: {info['n_units_test']}\")\n",
    "\n",
    "# Check if all have same number of features\n",
    "n_features = [info['n_features'] for info in compatibility.values()]\n",
    "if len(set(n_features)) == 1:\n",
    "    print(f\"\\n✓ All datasets have {n_features[0]} features - COMPATIBLE\")\n",
    "    can_combine = True\n",
    "else:\n",
    "    print(f\"\\n✗ Datasets have different feature counts: {set(n_features)} - NOT COMPATIBLE\")\n",
    "    can_combine = False\n",
    "\n",
    "# Step 2: Process multiple datasets (only if compatible)\n",
    "if can_combine:\n",
    "    # Select which files to combine (example: first 3 files)\n",
    "    files_to_process = all_files[:3]  # Adjust as needed\n",
    "    \n",
    "    print(f\"\\nStep 2: Processing {len(files_to_process)} datasets...\")\n",
    "    all_units_data, all_labels, test_units_data, test_labels = process_multiple_datasets(\n",
    "        files_to_process,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        stride=STRIDE\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create train/val split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xtrain, xval, ytrain, yval = train_test_split(\n",
    "        all_units_data, all_labels, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    ytrain = ytrain.reshape([-1, 1])\n",
    "    yval = yval.reshape([-1, 1])\n",
    "    test_labels = test_labels.reshape([-1, 1])\n",
    "    \n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"  Train: {xtrain.shape}\")\n",
    "    print(f\"  Val: {xval.shape}\")\n",
    "    print(f\"  Test: {test_units_data.shape}\")\n",
    "    \n",
    "    # Continue with your existing tf.data pipeline and training...\n",
    "else:\n",
    "    print(\"\\n⚠ Datasets are not compatible for combining.\")\n",
    "    print(\"Consider training separate models for each dataset.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
